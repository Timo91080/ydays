/**
 * Lab 2 - Comparaison Local vs API
 * Compare les performances de modÃ¨les locaux (Ollama) vs API cloud (Azure OpenAI)
 */

import { AzureOpenAI } from "openai";
import dotenv from "dotenv";

dotenv.config();

// Question de test commune
const TEST_PROMPT = "Explique-moi ce qu'est un agent d'IA.";

// RÃ©sultats de la comparaison
const results = [];

/**
 * Teste un modÃ¨le local via Ollama
 */
async function testOllamaModel(modelName) {
  console.log(`\nğŸ”¬ Test: ${modelName} (Ollama Local)...`);

  const startTime = Date.now();

  try {
    const response = await fetch("http://localhost:11434/api/generate", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: modelName,
        prompt: TEST_PROMPT,
        stream: false
      })
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`);
    }

    const data = await response.json();
    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(1);

    console.log(`âœ… TerminÃ© en ${duration}s`);
    console.log(`ğŸ“ RÃ©ponse (${data.response.length} caractÃ¨res):`);
    console.log(data.response.substring(0, 200) + "...\n");

    results.push({
      modele: modelName,
      mode: "Local (Ollama)",
      temps: duration + "s",
      longueur: data.response.length,
      reponse: data.response,
      error: null
    });

    return data.response;
  } catch (error) {
    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(1);

    console.log(`âŒ Erreur: ${error.message}`);

    results.push({
      modele: modelName,
      mode: "Local (Ollama)",
      temps: duration + "s",
      longueur: 0,
      reponse: "",
      error: error.message
    });

    return null;
  }
}

/**
 * Teste Azure OpenAI (GPT-4)
 */
async function testAzureOpenAI() {
  console.log(`\nğŸ”¬ Test: GPT-4 (Azure OpenAI API)...`);

  const startTime = Date.now();

  try {
    const endpoint = process.env.AZURE_OPENAI_ENDPOINT;
    const apiKey = process.env.AZURE_OPENAI_API_KEY;
    const apiVersion = process.env.AZURE_OPENAI_API_VERSION;
    const deployment = process.env.AZURE_OPENAI_DEPLOYMENT;

    const client = new AzureOpenAI({ endpoint, apiKey, deployment, apiVersion });

    const response = await client.chat.completions.create({
      messages: [
        { role: "user", content: TEST_PROMPT }
      ],
      max_completion_tokens: 500
    });

    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(1);

    const content = response.choices[0].message.content;

    console.log(`âœ… TerminÃ© en ${duration}s`);
    console.log(`ğŸ“ RÃ©ponse (${content.length} caractÃ¨res):`);
    console.log(content.substring(0, 200) + "...\n");

    results.push({
      modele: "GPT-4",
      mode: "API (Azure)",
      temps: duration + "s",
      longueur: content.length,
      reponse: content,
      tokens: response.usage.total_tokens,
      error: null
    });

    return content;
  } catch (error) {
    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(1);

    console.log(`âŒ Erreur: ${error.message}`);

    results.push({
      modele: "GPT-4",
      mode: "API (Azure)",
      temps: duration + "s",
      longueur: 0,
      reponse: "",
      error: error.message
    });

    return null;
  }
}

/**
 * Affiche le tableau de comparaison
 */
function displayComparison() {
  console.log("\n" + "=".repeat(80));
  console.log("ğŸ“Š TABLEAU DE COMPARAISON - Lab 2");
  console.log("=".repeat(80));
  console.log("\nQuestion posÃ©e:", TEST_PROMPT);
  console.log("\n");

  // En-tÃªte du tableau
  console.log("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
  console.log("â”‚ ModÃ¨le          â”‚ Mode             â”‚ Temps    â”‚ Longueur   â”‚ QualitÃ©     â”‚");
  console.log("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

  // Lignes du tableau
  results.forEach(result => {
    if (result.error) {
      console.log(`â”‚ ${pad(result.modele, 15)} â”‚ ${pad(result.mode, 16)} â”‚ ${pad(result.temps, 8)} â”‚ ${pad("ERREUR", 10)} â”‚ ${pad("N/A", 11)} â”‚`);
    } else {
      console.log(`â”‚ ${pad(result.modele, 15)} â”‚ ${pad(result.mode, 16)} â”‚ ${pad(result.temps, 8)} â”‚ ${pad(result.longueur + " car", 10)} â”‚ ${pad("Ã€ Ã©valuer", 11)} â”‚`);
    }
  });

  console.log("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

  console.log("\nğŸ“ OBSERVATIONS:");
  console.log("\nAnalyse du style de rÃ©ponse pour chaque modÃ¨le:\n");

  results.forEach((result, index) => {
    if (!result.error) {
      console.log(`${index + 1}. ${result.modele} (${result.mode}):`);
      console.log(`   Temps: ${result.temps}`);
      console.log(`   Style: ${analyzeStyle(result.reponse)}`);
      console.log(`   PremiÃ¨re ligne: "${result.reponse.split('\n')[0].substring(0, 60)}..."`);
      console.log("");
    }
  });

  console.log("\nğŸ’¡ RECOMMANDATIONS:");
  console.log("   â€¢ Ã‰valuez la qualitÃ© (1-5) selon:");
  console.log("     - PrÃ©cision et exactitude");
  console.log("     - ClartÃ© et lisibilitÃ©");
  console.log("     - Pertinence et complÃ©tude");
  console.log("   â€¢ Notez les diffÃ©rences de style et de formatage");
  console.log("   â€¢ Comparez le temps de rÃ©ponse");
  console.log("\n" + "=".repeat(80) + "\n");
}

/**
 * Analyse le style de la rÃ©ponse
 */
function analyzeStyle(text) {
  const hasLineBreaks = text.includes('\n\n');
  const hasBullets = /[â€¢\-\*]/.test(text);
  const hasNumbering = /^\d+\./.test(text);
  const avgWordLength = text.split(/\s+/).reduce((sum, word) => sum + word.length, 0) / text.split(/\s+/).length;

  const features = [];
  if (hasLineBreaks) features.push("paragraphes");
  if (hasBullets) features.push("puces");
  if (hasNumbering) features.push("numÃ©rotation");
  if (avgWordLength > 6) features.push("vocabulaire technique");

  return features.length > 0 ? features.join(", ") : "texte brut simple";
}

/**
 * Utilitaire pour padding
 */
function pad(str, length) {
  str = String(str);
  return str + " ".repeat(Math.max(0, length - str.length));
}

/**
 * Fonction principale
 */
async function main() {
  console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
  console.log("â•‘          Lab 2 - Comparaison Local vs API                     â•‘");
  console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

  // Test des modÃ¨les locaux Ollama
  console.log("\nğŸ“¦ PHASE 1: Test du modÃ¨le local (Ollama)");
  console.log("â”€".repeat(60));

  await testOllamaModel("mistral");

  // Test des API cloud
  console.log("\nâ˜ï¸  PHASE 2: Test de l'API cloud");
  console.log("â”€".repeat(60));

  await testAzureOpenAI();

  // Afficher les rÃ©sultats
  displayComparison();

  console.log("âœ… Lab 2 terminÃ©! Analysez les rÃ©sultats ci-dessus pour complÃ©ter votre tableau.");
}

// ExÃ©cution
main();
