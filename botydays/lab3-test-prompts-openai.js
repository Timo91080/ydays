/**
 * Lab 3 - Prompt Engineering avec OpenAI
 * Script pour tester et comparer diff√©rents prompts avec OpenAI standard
 */

import OpenAI from "openai";
import dotenv from "dotenv";
import fs from "fs";

dotenv.config();

// Configuration OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const modelName = process.env.OPENAI_MODEL || "gpt-4o-mini";

// Charger les prompts depuis le fichier JSON
const promptsData = JSON.parse(fs.readFileSync('./lab3-prompts.json', 'utf8'));
const results = [];

console.log("üß™ Lab 3 - Test de Prompt Engineering\n");
console.log("‚ïê".repeat(80));

async function testerPrompt(promptConfig) {
  console.log(`\nüìù Test : ${promptConfig.nom}`);
  console.log(`Description : ${promptConfig.description}`);
  console.log("‚îÄ".repeat(80));
  console.log(`Prompt :\n${promptConfig.prompt}`);
  console.log("‚îÄ".repeat(80));

  try {
    const startTime = Date.now();

    const response = await openai.chat.completions.create({
      model: modelName,
      messages: [
        { role: "user", content: promptConfig.prompt }
      ],
      max_tokens: 500,
      temperature: 0.7
    });

    const endTime = Date.now();
    const duration = endTime - startTime;

    const reponse = response.choices[0].message.content;
    const tokens = response.usage.total_tokens;

    console.log(`\n‚úÖ R√©ponse (${duration}ms, ${tokens} tokens) :\n`);
    console.log(reponse);
    console.log("\n" + "‚ïê".repeat(80));

    // Enregistrer les r√©sultats
    results.push({
      id: promptConfig.id,
      nom: promptConfig.nom,
      description: promptConfig.description,
      prompt: promptConfig.prompt,
      reponse: reponse,
      tokens: tokens,
      duree_ms: duration,
      observations: analyserReponse(reponse)
    });

  } catch (error) {
    console.error(`‚ùå Erreur : ${error.message}\n`);
    console.log("‚ïê".repeat(80));
  }
}

function analyserReponse(reponse) {
  const observations = [];

  // Longueur
  const mots = reponse.split(/\s+/).length;
  observations.push(`${mots} mots`);

  // Structure
  if (reponse.includes('\n\n')) {
    const paragraphes = reponse.split('\n\n').filter(p => p.trim().length > 0).length;
    observations.push(`${paragraphes} paragraphes`);
  }

  // Listes
  if (reponse.includes('‚Ä¢') || reponse.includes('-') || /^\d+\./.test(reponse)) {
    observations.push("Format liste");
  }

  // Exemples
  if (reponse.toLowerCase().includes('exemple') || reponse.toLowerCase().includes('par exemple')) {
    observations.push("Contient des exemples");
  }

  // Analogies/M√©taphores
  if (reponse.toLowerCase().includes('comme') || reponse.toLowerCase().includes('tel que')) {
    observations.push("Utilise des analogies");
  }

  return observations.join(', ');
}

async function executerTests() {
  console.log(`Mod√®le utilis√© : ${modelName}`);
  console.log(`Nombre de prompts √† tester : ${promptsData.prompts.length}\n`);

  // Tester tous les prompts
  for (const prompt of promptsData.prompts) {
    await testerPrompt(prompt);
    // Petite pause entre les requ√™tes
    await new Promise(resolve => setTimeout(resolve, 1000));
  }

  // G√©n√©rer le tableau comparatif
  genererTableauComparatif();

  // Sauvegarder les r√©sultats en JSON
  fs.writeFileSync('./lab3-resultats.json', JSON.stringify(results, null, 2), 'utf8');
  console.log("\nüíæ R√©sultats sauvegard√©s dans lab3-resultats.json");
}

function genererTableauComparatif() {
  console.log("\n\nüìä TABLEAU COMPARATIF");
  console.log("‚ïê".repeat(120));
  console.log(
    "Prompt".padEnd(30) + " | " +
    "Tokens".padEnd(8) + " | " +
    "Dur√©e".padEnd(10) + " | " +
    "Observations"
  );
  console.log("‚ïê".repeat(120));

  results.forEach(result => {
    console.log(
      result.nom.substring(0, 29).padEnd(30) + " | " +
      result.tokens.toString().padEnd(8) + " | " +
      `${result.duree_ms}ms`.padEnd(10) + " | " +
      result.observations
    );
  });

  console.log("‚ïê".repeat(120));

  // G√©n√©rer le fichier Markdown
  genererMarkdown();
}

function genererMarkdown() {
  let markdown = "# Lab 3 - R√©sultats du Prompt Engineering\n\n";
  markdown += `**Mod√®le test√© :** ${modelName}\n\n`;
  markdown += `**Date :** ${new Date().toLocaleDateString('fr-FR')}\n\n`;
  markdown += "---\n\n";

  results.forEach((result, index) => {
    markdown += `## ${index + 1}. ${result.nom}\n\n`;
    markdown += `**Description :** ${result.description}\n\n`;
    markdown += `**Prompt :**\n\`\`\`\n${result.prompt}\n\`\`\`\n\n`;
    markdown += `**R√©ponse :**\n${result.reponse}\n\n`;
    markdown += `**M√©triques :**\n`;
    markdown += `- Tokens utilis√©s : ${result.tokens}\n`;
    markdown += `- Dur√©e : ${result.duree_ms}ms\n`;
    markdown += `- Observations : ${result.observations}\n\n`;
    markdown += "---\n\n";
  });

  markdown += "## Tableau Comparatif\n\n";
  markdown += "| Prompt | Tokens | Dur√©e | Observations |\n";
  markdown += "|--------|--------|-------|-------------|\n";

  results.forEach(result => {
    markdown += `| ${result.nom} | ${result.tokens} | ${result.duree_ms}ms | ${result.observations} |\n`;
  });

  markdown += "\n## Analyse\n\n";
  markdown += "### Conclusions\n\n";
  markdown += "1. **Impact du r√¥le** : L'ajout d'un r√¥le permet d'orienter le ton et le style de la r√©ponse.\n";
  markdown += "2. **Structure compl√®te** : Les prompts structur√©s (R√¥le + Contexte + T√¢che + Format) donnent des r√©ponses plus pr√©cises et adapt√©es.\n";
  markdown += "3. **Formats sp√©cifiques** : Demander explicitement un format (liste, paragraphes) am√©liore la structuration de la r√©ponse.\n";

  fs.writeFileSync('./lab3-resultats.md', markdown, 'utf8');
  console.log("üìÑ Rapport Markdown g√©n√©r√© : lab3-resultats.md\n");
}

// Ex√©cuter les tests
executerTests().catch(error => {
  console.error("Erreur lors de l'ex√©cution des tests:", error);
});
